{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (mAP)\n",
    "\n",
    "The most important metric used to evaluate object-detection models.\n",
    "\n",
    "mAP is a comprehensive metric that combines both precision and recall across all object classes in a detection system. Here's a breakdown of how it works:\n",
    "\n",
    "1. **Precision and Recall**\n",
    "   - **Precision**: The percentage of correct detections among all detections made.(compared with predictions)\n",
    "   - **Recall**: The percentage of actual objects that were correctly detected from the image.(compared with ground truth)\n",
    "\n",
    "2. **Average Precision (AP)**\n",
    "   - For each class, AP calculates the area under the precision-recall curve\n",
    "   - It's computed by varying the confidence threshold and plotting precision vs recall\n",
    "   - A higher AP means the model is better at detecting that specific class\n",
    "\n",
    "3. **Mean Average Precision (mAP)**\n",
    "   - mAP is simply the mean of AP values across all classes\n",
    "   - Formula: mAP = (AP1 + AP2 + ... + APn) / n\n",
    "   - Where n is the number of classes\n",
    "\n",
    "### How it's Calculated\n",
    "\n",
    "1. **For each class**:\n",
    "   - Sort all detections by confidence score (as shown in the non-max suppression code)\n",
    "\n",
    "2. **For each detection**:\n",
    "   - Calculate IoU with ground truth boxes\n",
    "   - A detection is considered correct if IoU > threshold (typically 0.5)\n",
    "   - Update precision and recall values\n",
    "\n",
    "3. **Compute AP**:\n",
    "   - Plot precision vs recall curve\n",
    "   - Calculate area under the curve\n",
    "   - This gives AP for one class\n",
    "\n",
    "4. **Calculate mAP**:\n",
    "   - Take the mean of all class APs\n",
    "\n",
    "### Why mAP is Important\n",
    "\n",
    "1. **Comprehensive Evaluation**: Considers both precision and recall\n",
    "2. **Class Balance**: Equally weights performance across all classes\n",
    "3. **Industry Standard**: Widely used in competitions (COCO, Pascal VOC) and research\n",
    "4. **Confidence Threshold Independent**: Evaluates model performance across all confidence thresholds\n",
    "\n",
    "The metric is particularly useful because it:\n",
    "- Penalizes both false positives and false negatives\n",
    "- Accounts for confidence scores of predictions\n",
    "- Handles multi-class detection scenarios\n",
    "- Provides a single number to compare different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def intersection_over_union(predictions,labels,format):\n",
    "  '''\n",
    "  Calculates intersection over union \n",
    "  \n",
    "  Parameters:\n",
    "    predictions: predictions of bounding boxes\n",
    "    lables: correct labels of boxes\n",
    "    format: midpoint/corners (x,y,w,h) or (x1,y1,x2,y2)\n",
    "  '''\n",
    "\n",
    "  if format == \"midpoint\":\n",
    "      box1_x1 = predictions[..., 0:1] - predictions[..., 2:3] / 2\n",
    "      box1_y1 = predictions[..., 1:2] - predictions[..., 3:4] / 2\n",
    "      box1_x2 = predictions[..., 0:1] + predictions[..., 2:3] / 2\n",
    "      box1_y2 = predictions[..., 1:2] + predictions[..., 3:4] / 2\n",
    "      box2_x1 = labels[..., 0:1] - labels[..., 2:3] / 2\n",
    "      box2_y1 = labels[..., 1:2] - labels[..., 3:4] / 2\n",
    "      box2_x2 = labels[..., 0:1] + labels[..., 2:3] / 2\n",
    "      box2_y2 = labels[..., 1:2] + labels[..., 3:4] / 2\n",
    "\n",
    "  elif format == \"corners\":\n",
    "      box1_x1 = predictions[..., 0:1]\n",
    "      box1_y1 = predictions[..., 1:2]\n",
    "      box1_x2 = predictions[..., 2:3]\n",
    "      box1_y2 = predictions[..., 3:4]\n",
    "      box2_x1 = labels[..., 0:1]\n",
    "      box2_y1 = labels[..., 1:2]\n",
    "      box2_x2 = labels[..., 2:3]\n",
    "      box2_y2 = labels[..., 3:4]\n",
    "\n",
    "  x1 = torch.max(box1_x1, box2_x1)\n",
    "  y1 = torch.max(box1_y1, box2_y1)\n",
    "  x2 = torch.min(box1_x2, box2_x2)\n",
    "  y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "  # clamp(0) for boxes with no intersection  \n",
    "  intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "  box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "  box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "  return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        class_detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # append current class class_detections\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                class_detections.append(detection)\n",
    "\n",
    "        # append current class lables\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # count boxes for each image lable\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        class_detections.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        TP = torch.zeros((len(class_detections)))\n",
    "        FP = torch.zeros((len(class_detections)))\n",
    "\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(class_detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # this box is detected by the model\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "\n",
    "        # append current class average precision\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
