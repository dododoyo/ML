{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We do not need the exact cut out copy of the image to detect it in the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_display(img,cmap='gray'):\n",
    "  fig = plt.figure(figsize=(12,10))\n",
    "  ax = fig.add_subplot(111)\n",
    "  '''\n",
    "   This line adds a single subplot to the figure. The  method creates a subplot grid with 1 row and 1 column, and the subplot is placed in the first position.\n",
    "  '''\n",
    "  ax.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pictures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reeses = cv.imread('../data/Files/DATA/reeses_puffs.png',0)\n",
    "all_cereals = cv.imread('../data/Files/DATA/many_cereals.jpg',0)\n",
    "\n",
    "custom_display(all_cereals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force with ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORB = cv.ORB_create()\n",
    "\n",
    "'''\n",
    "kp is list of keypoints detected on an image \n",
    "des is the description of the point for each image \n",
    "'''\n",
    "kp1,des1 = ORB.detectAndCompute(reeses,None)\n",
    "kp2,des2 = ORB.detectAndCompute(all_cereals,None)\n",
    "\n",
    "bruteForce_matcher = cv.BFMatcher(cv.NORM_HAMMING,crossCheck=True)\n",
    "matches = bruteForce_matcher.match(des1,des2)\n",
    "\n",
    "# Tuple of Matches object\n",
    "print(matches)\n",
    "\n",
    "# distance atribute calrifies how good of a match it was \n",
    "# if the distance is less it means more match (i.e difference)\n",
    "matches = sorted(matches,key=lambda match : match.distance)\n",
    "\n",
    "reeses_matches = cv.drawMatches(reeses,kp1,all_cereals,kp2,matches[:25],None,flags=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet uses the [`cv.drawMatches`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A17%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\") function from the OpenCV library to visualize the matches between keypoints detected in two images. Let's break down the code and understand its components:\n",
    "\n",
    "1. **Function Call**:\n",
    "   ```python\n",
    "   reeses_matches = cv.drawMatches(reeses, kp1, all_cereals, kp2, matches[:25], None, flags=2)\n",
    "   ```\n",
    "   This line calls the [`cv.drawMatches`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A17%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\") function to draw the matches between keypoints in the two images. The function takes several parameters to specify the images, keypoints, matches, and drawing options.\n",
    "\n",
    "2. **Parameters**:\n",
    "   - [`reeses`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A32%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"): The first image, which contains the object of interest (e.g., a box of Reese's Puffs cereal).\n",
    "   - [`kp1`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A39%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"): The list of keypoints detected in the first image ([`reeses`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A32%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\")). These keypoints represent distinctive features in the image.\n",
    "   - [`all_cereals`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A43%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"): The second image, which contains multiple objects (e.g., a shelf with various cereal boxes).\n",
    "   - [`kp2`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A55%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"): The list of keypoints detected in the second image ([`all_cereals`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A43%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\")). These keypoints represent distinctive features in the image.\n",
    "   - [`matches[:25]`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A59%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"): A list of the top 25 matches between the keypoints in the two images. The [`matches`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A59%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\") list contains `DMatch` objects, each representing a pair of matched keypoints.\n",
    "   - [`None`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A72%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"): This parameter specifies the output image. When set to [`None`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A72%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"), the function creates a new image to draw the matches.\n",
    "   - [`flags=2`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A77%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\"): This parameter specifies drawing options. The value [`2`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A83%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\") corresponds to the [`cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A17%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\") flag, which means that only the matched keypoints will be drawn, and single keypoints will not be displayed.\n",
    "\n",
    "3. **Output**:\n",
    "   The [`cv.drawMatches`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A17%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\") function returns an image ([`reeses_matches`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A0%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\")) that shows the two input images side by side with lines connecting the matched keypoints. This visual representation helps in understanding how well the keypoints from the two images match and provides a clear way to verify the accuracy of the feature matching process.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The code snippet demonstrates how to use the [`cv.drawMatches`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Fdododoyo%2FDocuments%2FML%2F03-computer-vision%2FFeatureMatching.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22X12sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A19%2C%22character%22%3A17%7D%7D%5D%2C%2276755bff-4238-44cb-bcd4-a33d5933f1ea%22%5D \"Go to definition\") function to visualize the matches between keypoints detected in two images. By specifying the images, keypoints, and matches, the function creates an output image that highlights the matched keypoints with lines connecting them. This visualization is useful for evaluating the performance of feature detection and matching algorithms in computer vision tasks such as object recognition and image stitching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_display(reeses_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT Feature Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT (Scale-Invariant Feature Transform) is a widely used algorithm in computer vision for detecting and describing local features in images. It was developed by David Lowe in 1999 and has since become one of the most popular and robust methods for feature detection and matching. SIFT is particularly known for its ability to detect distinctive keypoints that are invariant to scale, rotation, and partially invariant to illumination changes and affine transformations.\n",
    "\n",
    "### Key Concepts of SIFT\n",
    "\n",
    "1. **Scale-Invariance**: SIFT can detect features at different scales, making it robust to changes in the size of the object in the image.\n",
    "2. **Rotation-Invariance**: SIFT features are invariant to rotation, meaning that the algorithm can detect the same features even if the object is rotated.\n",
    "3. **Distinctive Keypoints**: SIFT identifies keypoints that are highly distinctive, allowing for reliable matching between different images of the same object or scene.\n",
    "\n",
    "### Steps of the SIFT Algorithm\n",
    "\n",
    "1. **Scale-Space Extrema Detection**: The algorithm searches for keypoints over multiple scales using a difference-of-Gaussian (DoG) function. This involves creating a series of blurred images (octaves) and subtracting adjacent blurred images to find potential keypoints.\n",
    "\n",
    "2. **Keypoint Localization**: Potential keypoints are refined to sub-pixel accuracy. Keypoints with low contrast or those that are poorly localized along edges are discarded.\n",
    "\n",
    "3. **Orientation Assignment**: Each keypoint is assigned one or more orientations based on the local image gradient directions. This step ensures that the keypoints are rotation-invariant.\n",
    "\n",
    "4. **Keypoint Descriptor**: A 128-dimensional descriptor is computed for each keypoint. This descriptor is based on the local image gradients around the keypoint and is designed to be robust to small changes in illumination and viewpoint.\n",
    "\n",
    "5. **Keypoint Matching**: The descriptors of keypoints from different images are compared to find matches. Typically, a nearest-neighbor search is used to identify the best matches based on the Euclidean distance between descriptors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIFT = cv.SIFT_create()\n",
    "\n",
    "sift_kp1,sift_des1 = SIFT.detectAndCompute(reeses,None)\n",
    "sift_kp2,sift_des2 = SIFT.detectAndCompute(all_cereals,None)\n",
    "\n",
    "\n",
    "SIFT_brute_force_matcher = cv.BFMatcher()\n",
    "SIFT_matches = SIFT_brute_force_matcher.knnMatch(sift_des1,sift_des2,k=2)\n",
    "\n",
    "good_match = []\n",
    "\n",
    "for match1,match2 in SIFT_matches:\n",
    "  if match1.distance < 0.75*match2.distance:\n",
    "    good_match.append([match1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if `match1.distance < 0.75 * match2.distance` because we want to ensure that the best match (`match1`) is significantly better than the second-best match (`match2`). This is known as **Lowe's ratio test**, a technique used in feature matching to reduce the number of false matches.\n",
    "\n",
    "Here's why:\n",
    "\n",
    "- **`match1.distance`** represents the distance (difference) between a feature descriptor from the first image and its closest match in the second image.\n",
    "- **`match2.distance`** represents the distance to the second-closest match.\n",
    "\n",
    "By checking if `match1.distance` is less than 75% of `match2.distance`, we're verifying that the best match is substantially better than the second-best. This means the match is more likely to be correct and not due to noise or repetitive patterns.\n",
    "\n",
    "If we were to check `match2.distance < 0.75 * match1.distance`, we'd be asking if the second-best match is better than the best match, which doesn't align with the goal of selecting the most reliable matches.\n",
    "\n",
    "**In summary**, the condition `match1.distance < 0.75 * match2.distance` helps us keep only the matches where the best match is significantly better than the alternative, increasing the accuracy of our feature matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_matches = cv.drawMatchesKnn(reeses,sift_kp1,all_cereals,sift_kp2,good_match,None,flags=2)\n",
    "\n",
    "custom_display(sift_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotated Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Rotated Reeses Puffs\n",
    "rotated_reeses = cv.imread('../data/Files/DATA/rotated_reeses_puffs.png', 0)\n",
    "\n",
    "rotated_SIFT = cv.SIFT_create()\n",
    "\n",
    "rotated_SIFT_kp1, rotated_SIFT_des1 = rotated_SIFT.detectAndCompute(rotated_reeses, None)\n",
    "rotated_SIFT_kp2, rotated_SIFT_des2 = rotated_SIFT.detectAndCompute(all_cereals, None)\n",
    "\n",
    "rotated_SIFT_brute_force_matcher = cv.BFMatcher()\n",
    "rotated_SIFT_matches = rotated_SIFT_brute_force_matcher.knnMatch(rotated_SIFT_des1, rotated_SIFT_des2, k=2)\n",
    "\n",
    "rotated_good_match = []\n",
    "\n",
    "for match1, match2 in rotated_SIFT_matches:\n",
    "  if match1.distance < 0.75*match2.distance:\n",
    "    rotated_good_match.append([match1])\n",
    "\n",
    "\n",
    "rotated_sift_matches = cv.drawMatchesKnn(\n",
    "    rotated_reeses, rotated_SIFT_kp1, all_cereals, rotated_SIFT_kp2, rotated_good_match, None, flags=2)\n",
    "\n",
    "custom_display(rotated_sift_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🧔🏻‍♀️ Greate Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FLANN Based Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flann = cv.SIFT_create()\n",
    "\n",
    "flann_kp1_,flann_des1 = flann.detectAndCompute(reeses,None)\n",
    "flann_kp2_,flann_des2 = flann.detectAndCompute(all_cereals,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE,trees=5)\n",
    "\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann_matcher = cv.FlannBasedMatcher(indexParams=index_params,searchParams=search_params)\n",
    "\n",
    "flann_matches = flann_matcher.knnMatch(flann_des1,flann_des2,k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter good matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_flann_match = []\n",
    "\n",
    "for match1, match2 in flann_matches:\n",
    "  if match1.distance < 0.75*match2.distance:\n",
    "    good_flann_match.append([match1])\n",
    "\n",
    "good_matches = cv.drawMatchesKnn(reeses,flann_kp1_,all_cereals,flann_kp2_,good_flann_match,None,flags=0)\n",
    "\n",
    "custom_display(good_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotated FLANN Based Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_flann = cv.SIFT_create()\n",
    "\n",
    "rotated_flann_kp1_, rotated_flann_des1 = rotated_flann.detectAndCompute(rotated_reeses, None)\n",
    "rotated_flann_kp2_, rotated_flann_des2 = rotated_flann.detectAndCompute(all_cereals,None)\n",
    "\n",
    "index_params = dict(algorithm=0, trees=5)\n",
    "\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "rotated_flann_matcher = cv.FlannBasedMatcher(\n",
    "    indexParams=index_params, searchParams=search_params)\n",
    "\n",
    "rotated_flann_matches = rotated_flann_matcher.knnMatch(rotated_flann_des1, rotated_flann_des2, k=2)\n",
    "\n",
    "good_rotated_flann_match = []\n",
    "\n",
    "for match1, match2 in rotated_flann_matches:\n",
    "  if match1.distance < 0.75*match2.distance:\n",
    "    good_rotated_flann_match.append([match1])\n",
    "\n",
    "rotated_good_matches = cv.drawMatchesKnn(\n",
    "    rotated_reeses, rotated_flann_kp1_, all_cereals, rotated_flann_kp2_, good_rotated_flann_match, None, flags=0)\n",
    "\n",
    "custom_display(rotated_good_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw Matching Using Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchesMask = [[0,0] for i in range(len(flann_matches))]\n",
    "\n",
    "for i, (match1,match2) in enumerate(flann_matches):\n",
    "  if match1.distance < 0.7*match2.distance:\n",
    "    matchesMask[i] = [1,0]\n",
    "\n",
    "drawingParams = dict(matchColor=(0,255,0),singlePointColor=(255,0,0),matchesMask=matchesMask,flags=0)\n",
    "\n",
    "mask_flann_matches = cv.drawMatchesKnn(\n",
    "    reeses, flann_kp1_, all_cereals, flann_kp2_, flann_matches, None, **drawingParams)\n",
    "\n",
    "custom_display(mask_flann_matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
